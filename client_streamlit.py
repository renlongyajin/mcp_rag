import os
import sys
import time
from typing import Optional, Tuple, Type, Union

import requests
import streamlit as st
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.tools import BaseTool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from nltk.tokenize import sent_tokenize
from pydantic import BaseModel, Field

from config.config import global_config
from langchain.agents.output_parsers.tools import ToolAgentAction
from langchain_community.tools.tavily_search import TavilySearchResults  # TAVILYå·¥å…·
import json
from typing import Dict, Any

# Streamlité¡µé¢é…ç½®
st.set_page_config(
    page_title="MCPçŸ¥è¯†åº“åŠ©æ‰‹",
    page_icon=":books:",
    layout="centered"
)

st.write("""
<style>
/* ä¸åŒæ¥æºçš„è§†è§‰åŒºåˆ† */
div[data-testid="stMarkdownContainer"] ul {
    position: relative;
    padding-left: 1.5em;
}

div[data-testid="stMarkdownContainer"] ul:before {
    content: "";
    position: absolute;
    left: 0;
    top: 0.4em;
    height: 80%;
    width: 2px;
    background: linear-gradient(#2ecc71, #3498db);
}

/* ç½‘ç»œç»“æœæ ·å¼ */
.web-result {
    border-left: 3px solid #3498db;
    padding-left: 1rem;
    margin: 1rem 0;
}

/* çŸ¥è¯†åº“ç»“æœæ ·å¼ */
.kb-result {
    border-left: 3px solid #2ecc71;
    padding-left: 1rem;
    margin: 1rem 0;
}
</style>
""", unsafe_allow_html=True)

st.write("""
<style>
.md-box {
    padding: 1.2rem;
    border-left: 4px solid #2ecc71;
    background: #f8f9fa;
    border-radius: 0 8px 8px 0;
}
</style>
""", unsafe_allow_html=True)

# # å¼ºåˆ¶ç¦ç”¨ç¼“å­˜
# @st.cache_resource(ttl=1)  # 1ç§’ç¼“å­˜å‘¨æœŸ
# def get_client():
#     return global_config.sync_proxy_client

DEFAULT_ENDPOINT = "http://localhost:8123/search"


class MCPQueryInput(BaseModel):
    query: str = Field(description="æœç´¢æŸ¥è¯¢å†…å®¹")
    top_k: int = Field(default=3, description="è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡")
    knowledge_name: Optional[str] = Field(default=None)
    no_chunk: Optional[bool] = Field(default=False)


class MCPRAGTool(BaseTool):
    name: str = "mcp_rag_search"
    description: str = "å½“éœ€è¦æŸ¥è¯¢ç‰¹å®šé¢†åŸŸçŸ¥è¯†æˆ–æœ€æ–°ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤å·¥å…·"
    # ä¿®æ­£args_schemaçš„ç±»å‹æ³¨è§£
    args_schema: Type[BaseModel] = MCPQueryInput

    def _run(self, **params) -> str:
        response = requests.post(
            DEFAULT_ENDPOINT,
            json={k: v for k, v in params.items() if v is not None},
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        if not response.ok:
            return (f"æŸ¥è¯¢å¤±è´¥: {response.text}", {})

        raw_data = response.json()
        formatted = self._format_response(raw_data)
        return (formatted, {"raw_results": raw_data})

        # return self._format_response(response) if response.ok else f"æŸ¥è¯¢å¤±è´¥: {response.text}"
    def _format_response(self, raw_data: dict) -> str:
        """ç”ŸæˆåŒ…å«ç»“æ„åŒ–æ•°æ®çš„Markdownæ ¼å¼å­—ç¬¦ä¸²"""
        results = []
        for idx, item in enumerate(raw_data.get("results", []), 1):  # åªå–å‰3ä¸ªç»“æœ
            path = item.get('path', 'æ— è·¯å¾„ä¿¡æ¯')
            content = str(item.get('excerpt', '')).strip()
            
            results.append(
                f"### ç»“æœ {idx}\n"
                f"**è·¯å¾„**: `{path}`\n"
                f"**å†…å®¹**: {content}\n"
                "---"
            )
        return "\n\n".join(results) if results else "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
    # def _format_response(self, response) -> str:
    #     results = [
    #         f"æ–‡ä»¶è·¯å¾„: {item.get('path', 'æ— ')}\nå†…å®¹: {item.get('excerpt', 'æ— ')}"
    #         for item in response.json().get("results", [])
    #     ]
    #     return "\n\n".join(results) if results else "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"

def create_mcp_agent():
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3,
                     http_client=global_config.sync_proxy_client)
    # llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    tools = [MCPRAGTool(),TavilySearchResults(max_results=3)]
    my_prompt_template = """# æŒ‡ä»¤ç³»ç»Ÿ v2.1
                            ## çŸ¥è¯†åº“é…ç½®
                            - å½“å‰åº“ï¼š{knowledge_name} 
                            - è¿”å›æ•°ï¼š{top_k}

                            ## å¼ºåˆ¶æµç¨‹
                            1. é¦–æ¬¡å¿…é¡»ä½¿ç”¨`mcp_rag_search`æŸ¥è¯¢ï¼ˆå‚æ•°å¿…é¡»åŒ…å«ï¼šquery+knowledge_name+top_kï¼‰
                            2. å½“ä¸”ä»…å½“å‡ºç°ä»¥ä¸‹æƒ…å†µæ—¶ä½¿ç”¨`web_search`ï¼š
                            - æ ¹æ®çŸ¥è¯†åº“çš„å›ç­”æ— æ³•å‡†ç¡®å›ç­”é—®é¢˜ï¼Œç›¸å…³ä¿¡æ¯ä¸æ˜ç¡®æˆ–è€…æ‰¾ä¸åˆ°,å½“å‰é—®é¢˜ä¸º{question}
                            - ç”¨æˆ·æ˜ç¡®è¦æ±‚å®æ—¶ä¿¡æ¯
                            3. æœ€ç»ˆå›ç­”å¿…é¡»åŒ…å«ï¼š
                            - ğŸ“Œ æ¥æºæ ‡è®°ï¼ˆçŸ¥è¯†åº“/ç½‘ç»œï¼‰
                            - ğŸ” æ£€ç´¢å…³é”®è¯
                            - ğŸ“‚ çŸ¥è¯†åº“è·¯å¾„ï¼ˆè‹¥æœ€ç»ˆé‡‡ç”¨çŸ¥è¯†åº“åˆ™å¿…é¡»ç½—åˆ—å‚è€ƒè·¯å¾„ï¼‰
                            - ğŸ“‚ å¯¹åº”ç½‘å€ï¼ˆè‹¥æœ€ç»ˆé‡‡ç”¨ç½‘ç»œæœç´¢åˆ™å¿…é¡»ç½—åˆ—ç½‘å€ï¼‰

                            ## é”™è¯¯å¤„ç†
                            âŒ ç¦æ­¢åœ¨çŸ¥è¯†åº“æ— ç»“æœæ—¶è‡ªè¡Œæ¨ç†
                            âœ… å¿…é¡»é€šè¿‡å·¥å…·è·å–ç¡®åˆ‡ä¿¡æ¯"""

    
    prompt = ChatPromptTemplate.from_messages([
        ("system", my_prompt_template),
        MessagesPlaceholder("chat_history", optional=True),
        ("human", "{input}"),
        MessagesPlaceholder("agent_scratchpad")
    ])

    return AgentExecutor.from_agent_and_tools(
        agent=create_openai_tools_agent(llm, tools, prompt),
        tools=tools,
        verbose=True,
        return_intermediate_steps=True, #è¿”å›ä¸­é—´ç»“æœ
        handle_parsing_errors="è¯·é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜"
    )


def knowledge_base_selector():
    col1, col2, col3 = st.columns([0.6, 0.2, 0.2])  # è°ƒæ•´åˆ—å®½æ¯”ä¾‹
    with col1:
        global_config.knowledge_manager.load_knowledge_base()
        kb_list = getattr(global_config.knowledge_manager, 'all_knowledge_base', [])
        kb_names = [kb.knowledge_name for kb in kb_list]
        st.session_state.selected_kb = st.selectbox(
            "é€‰æ‹©çŸ¥è¯†åº“",
            options=kb_names,
            index=0
        )
    with col2:
        if st.button("ï¼‹", help="æ–°å»ºçŸ¥è¯†åº“"):
            st.session_state.show_create_kb = True

    with col3:
        st.session_state.top_k = st.number_input(
            "æœ€å¤§ç»“æœæ•°",
            min_value=1,
            max_value=20,
            value=3,
            step=1,
            help="è®¾ç½®è¿”å›ç»“æœçš„æœ€å¤§æ•°é‡"
        )

    if st.session_state.get("show_create_kb"):
        with st.expander("æ–°å»ºçŸ¥è¯†åº“", expanded=True):
            with st.form("create_kb_form"):
                name = st.text_input("åç§°")
                path = st.text_input("è·¯å¾„")
                description = st.text_input("çŸ¥è¯†åº“æè¿°")
                chunk_size = st.number_input("åˆ†å—å¤§å°", min_value=300, max_value=2000, value=1000, step=100,
                                             help="åˆ†å—å¤§å°æ˜¯æŒ‡å°†æ–‡ä»¶åˆ†å—å†è¿›è¡Œæ£€ç´¢ï¼Œåˆ†å—è¶Šå°ï¼Œæ£€ç´¢è¶Šç»†è‡´")
                chunk_overlap = st.number_input("åˆ†å—é‡å ", min_value=0, max_value=500, value=200, step=100,
                                                help="åˆ†å—é‡å æ˜¯æŒ‡æ¯ä¸¤ä¸ªåˆ†å—ä¹‹é—´çš„é‡å å¤§å°")
                if st.form_submit_button("åˆ›å»º") and name and path:
                    try:
                        global_config.knowledge_manager.create_knowledge_base(name, path, description, chunk_size,
                                                                              chunk_overlap)
                        st.success("åˆ›å»ºæˆåŠŸ")
                        st.session_state.show_create_kb = False
                        st.experimental_rerun()
                    except Exception as e:
                        st.error(f"åˆ›å»ºå¤±è´¥: {str(e)}")


def main():
    st.title("MCPä¸“ä¸šçŸ¥è¯†é—®ç­”ç³»ç»Ÿ :books:")
    knowledge_base_selector()

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "agent" not in st.session_state:
        st.session_state.agent = create_mcp_agent()
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).markdown(msg["content"])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥é—®é¢˜..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").markdown(prompt)

        with st.chat_message("assistant"):
            response = st.session_state.agent.invoke({
                "input": prompt,
                "question": prompt,
                "knowledge_name": st.session_state.selected_kb,
                # "kb_list": ", ".join([kb.knowledge_name for kb in global_config.knowledge_manager.all_knowledge_base]),
                "top_k": st.session_state.top_k
            })

            # æ˜¾ç¤ºä¸­é—´æ­¥éª¤
            if response.get("intermediate_steps"):
                with st.expander("ğŸ§  æ€è€ƒè¿‡ç¨‹", expanded=False):
                    for i, step in enumerate(response["intermediate_steps"], 1):
                        action, result = step[0], step[1]
                        
                        step_content = [f"### æ­¥éª¤ {i}"]
                        
                        if isinstance(action, ToolAgentAction): 
                            # å·¥å…·ç±»å‹æ ‡è¯†
                            tool_type = "è”ç½‘æœç´¢" if action.tool == "tavily_search_results_json" else "æœ¬åœ°çŸ¥è¯†åº“æŸ¥è¯¢"
                            step_content.append(f"**å·¥å…·ç±»å‹**: {tool_type}")
                            
                            # é€šç”¨ä¿¡æ¯å±•ç¤º
                            step_content.extend([
                                f"**å·¥å…·åç§°**: `{action.tool}`",
                                "**å‚æ•°**:",
                                "\n".join([f"- `{k}`: `{v}`" for k, v in action.tool_input.items()]),
                                f"**æ—¥å¿—**:\n```\n{action.log.strip()}\n```"
                            ])
                            
                            # ç»“æœç‰¹æ®Šå¤„ç†
                            step_content.append("**æ‰§è¡Œç»“æœ**:")
                            if action.tool == "tavily_search_results_json":
                                if isinstance(result, tuple):
                                    # å¤„ç†(content, raw_data)æ ¼å¼
                                    content, raw_data = result
                                    step_content.append(content)
                                    with st.expander("æŸ¥çœ‹åŸå§‹æ•°æ®", expanded=False):
                                        st.json(raw_data["raw_results"])
                                elif isinstance(result, str) and result.startswith("è”ç½‘æœç´¢å¤±è´¥"):
                                    step_content.append(f"âŒ {result}")
                                else:
                                    step_content.append(f"```\n{str(result)[:300]}\n```")
                            elif action.tool == "mcp_rag_search":
                                if isinstance(result, tuple) and len(result) == 2:
                                    content, raw_data = result
                                    step_content.append(content)
                                    # ä½¿ç”¨å®¹å™¨æ›¿ä»£expand
                               
                                    for idx, item in enumerate(raw_data.get("raw_results", {}).get("results", []), 1):
                                        step_content.append(f"**ç»“æœ** {idx}\n")
                                        step_content.append(f"è·¯å¾„: {item.get('path', 'æ— è·¯å¾„')}\n")
                                        step_content.append(f"å†…å®¹æ‘˜è¦: {str(item.get('excerpt', 'æ— å†…å®¹'))[:300]}\n")
                                        step_content.append("---")
                            
                            else:
                                step_content.append(f"```\n{str(result)[:300]}\n```")
                        
                        st.markdown("\n\n".join(step_content))
                        st.divider()
                        

            full_response = response.get("output", "æ— æ³•ç”Ÿæˆå›ç­”")
            display_text = ""
            placeholder = st.empty()

            sentences = sent_tokenize(full_response)
            display_text = ""

            for sent in sentences:
                display_text += sent + " "
                placeholder.markdown(display_text + "â–Œ", unsafe_allow_html=True)
                time.sleep(0.05)
            placeholder.markdown(display_text)

        st.session_state.messages.append({"role": "assistant", "content": full_response})


if __name__ == "__main__":
    main()
